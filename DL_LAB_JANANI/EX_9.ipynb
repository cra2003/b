{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adde1095-c007-4a12-ab19-b2e29bde1bd0",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf013b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205860bb-da21-4fd8-ad7a-93afea6c5dfe",
   "metadata": {},
   "source": [
    "## Loading the data into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a19783b-4dd3-4f6b-a284-272dd43734ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poems.txt', 'r') as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3f905-2585-4fe8-b848-e45357e93b94",
   "metadata": {},
   "source": [
    "## Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a71f14-2250-4011-9502-594b338d0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(text))\n",
    "char_to_index = {char: index for index, char in enumerate(unique_chars)}\n",
    "index_to_char = {index: char for index, char in enumerate(unique_chars)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb35813-4345-4aae-9a83-59b32138604c",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d059a291-7a1c-466c-bfef-85ecfd4add89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 36  # You can adjust this based on your preferences\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - max_sequence_length, 1):\n",
    "    input_sequence = text[i:i + max_sequence_length]\n",
    "    output_char = text[i + max_sequence_length]\n",
    "    sequences.append([char_to_index[char] for char in input_sequence])\n",
    "    next_chars.append(char_to_index[output_char])\n",
    "\n",
    "# Pad sequences to ensure they all have the same length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, dtype='int32')\n",
    "y = tf.keras.utils.to_categorical(next_chars, num_classes=len(unique_chars))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8c747-aa83-4bad-aa90-7fe33ddd3ac7",
   "metadata": {},
   "source": [
    "## GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd03035e-e087-4dd8-889a-6b12817f16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(len(unique_chars), 256, input_length=max_sequence_length),\n",
    "    GRU(1024, return_sequences=True),\n",
    "    GRU(1024),\n",
    "    Dense(len(unique_chars), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c5551-9938-498e-91fb-ee30b9378934",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d41dc35-32ce-42bd-9d0e-010f7bc54c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Helper function to sample the next character\n",
    "def sample_next_char(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# b) Helper function to generate text after each epoch\n",
    "def generate_text(seed_text, length=400, temperature=0.5):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(length):\n",
    "        encoded_text = [char_to_index[char] for char in generated_text[-max_sequence_length:]]\n",
    "        encoded_text = np.array([encoded_text])\n",
    "        predicted_probs = model.predict(encoded_text, verbose=0)[0]\n",
    "        next_index = sample_next_char(predicted_probs, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "        generated_text += next_char\n",
    "    return generated_text\n",
    "\n",
    "# c) Helper function to save the model after each epoch in which loss decreases\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# d) Helper function to reduce the learning rate each time the learning plateaus\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4175c5-cd0e-4ff4-b410-ac68b8883f30",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1983c3cb-851c-45ac-b22d-7258ee838215",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 3.6033\n",
      "Epoch 1: loss improved from inf to 3.60330, saving model to model.h5\n",
      "14/14 [==============================] - 48s 3s/step - loss: 3.6033 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 3.1296\n",
      "Epoch 2: loss improved from 3.60330 to 3.12964, saving model to model.h5\n",
      "14/14 [==============================] - 46s 3s/step - loss: 3.1296 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.8829\n",
      "Epoch 3: loss improved from 3.12964 to 2.88288, saving model to model.h5\n",
      "14/14 [==============================] - 45s 3s/step - loss: 2.8829 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.6182\n",
      "Epoch 4: loss improved from 2.88288 to 2.61820, saving model to model.h5\n",
      "14/14 [==============================] - 45s 3s/step - loss: 2.6182 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.4033\n",
      "Epoch 5: loss improved from 2.61820 to 2.40328, saving model to model.h5\n",
      "14/14 [==============================] - 46s 3s/step - loss: 2.4033 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2473\n",
      "Epoch 6: loss improved from 2.40328 to 2.24725, saving model to model.h5\n",
      "14/14 [==============================] - 47s 3s/step - loss: 2.2473 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.1233\n",
      "Epoch 7: loss improved from 2.24725 to 2.12328, saving model to model.h5\n",
      "14/14 [==============================] - 45s 3s/step - loss: 2.1233 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.9884\n",
      "Epoch 8: loss improved from 2.12328 to 1.98843, saving model to model.h5\n",
      "14/14 [==============================] - 33s 2s/step - loss: 1.9884 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8379\n",
      "Epoch 9: loss improved from 1.98843 to 1.83785, saving model to model.h5\n",
      "14/14 [==============================] - 15s 1s/step - loss: 1.8379 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6636\n",
      "Epoch 10: loss improved from 1.83785 to 1.66363, saving model to model.h5\n",
      "14/14 [==============================] - 32s 2s/step - loss: 1.6636 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4824\n",
      "Epoch 11: loss improved from 1.66363 to 1.48238, saving model to model.h5\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4824 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2496\n",
      "Epoch 12: loss improved from 1.48238 to 1.24960, saving model to model.h5\n",
      "14/14 [==============================] - 45s 3s/step - loss: 1.2496 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9771\n",
      "Epoch 13: loss improved from 1.24960 to 0.97714, saving model to model.h5\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.9771 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7202\n",
      "Epoch 14: loss improved from 0.97714 to 0.72016, saving model to model.h5\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.7202 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4944\n",
      "Epoch 15: loss improved from 0.72016 to 0.49438, saving model to model.h5\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4944 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7e9d66e20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=[checkpoint, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951cd27-7e3d-4f95-bc8c-fa401dfb4386",
   "metadata": {},
   "source": [
    "## Generating new and random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a106b507-7d9d-49e1-a480-eecf5d546ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And savraveled by,\n",
      "And that I stood my dreams undergrowth;\n",
      "\n",
      "Then took that mas mads mingerem bloth-as fare.\n",
      "\n",
      "The brag with crooked hands;\n",
      "The blages han\n",
      "and I ke tast the blothe ave spread my dreams;\n",
      "I have spread my dreams undergrowth;\n",
      "\n",
      "The blake all the dif a yellow with a stood,\n",
      "And sire a tho diverg in the was hade\n",
      "To way tha firs crag with crooked hands;\n",
      "Close tast the bloths undergrowth;\n",
      "\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Two roads diverged in a yellow wood,\"\n",
    "generated_poem = generate_text(seed_text, length=400, temperature=0.5)\n",
    "print(generated_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c13c1-285b-4062-ab7d-15d8ab99130c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
